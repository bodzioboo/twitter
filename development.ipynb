{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development\n",
    "This is a working file in which I develop different elements of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All libraries that I could possibly need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_cleaner import TweetCleaner\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import gc\n",
    "import operator\n",
    "import itertools\n",
    "import pdb\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import scipy as sp \n",
    "import collections\n",
    "import random\n",
    "import csv\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = json.load(open(\"cleaner.config\",\"r\"))\n",
    "tmp.pop('test/fala.csv')\n",
    "json.dump(tmp,open(\"cleaner.config\",\"w\"))\n",
    "!cd test && ls | grep fala_clean | xargs rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = TweetCleaner(\"cleaner.config\")\n",
    "begin = time.time()\n",
    "cleaner.clean(\"data/fala.csv\", \"test/fala_clean.csv\", \n",
    "              path_stopwords = \"nlp/polish.stopwords.txt\", chunk_size = 100000)\n",
    "end = time.time()\n",
    "print(f\"Runtime {end - begin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_data = pd.read_csv(\"data/clean/gov_tweets_clean_2020_03_29.csv\", header = 0, index_col = False, dtype = str)\n",
    "gov_data[\"source\"] = \"government\"\n",
    "gov_data = gov_data.loc[gov_data.created_at.notna()]\n",
    "gov_data = gov_data.drop(columns = [\"index\"])\n",
    "gov_data[\"created_at\"] = pd.to_datetime(gov_data[\"created_at\"], format = \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "gov_data[\"day\"] = gov_data.created_at.dt.to_period(\"D\")\n",
    "gov_data = gov_data.loc[gov_data[\"day\"] == pd.Period('2020-03-29', 'D')].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opp_data = pd.read_csv(\"data/clean/opp_tweets_2020_03_29.csv\", header = 0, index_col = False, dtype = str)\n",
    "opp_data[\"source\"] = \"opposition\"\n",
    "opp_data = opp_data.loc[opp_data.created_at.notna()]\n",
    "opp_data = opp_data.drop(columns = [\"index\"])\n",
    "opp_data[\"created_at\"] = pd.to_datetime(opp_data[\"created_at\"], format = \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "opp_data[\"day\"] = opp_data.created_at.dt.to_period(\"D\")\n",
    "opp_data = opp_data.loc[opp_data[\"day\"] == pd.Period('2020-03-29', 'D')].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data = pd.concat([gov_data, opp_data], axis = 0)\n",
    "del(gov_data)\n",
    "del(opp_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.dropna(subset = [\"source\",\"preprocessed\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(data, ngram_range, limit):\n",
    "    vectorizer = CountVectorizer(ngram_range = ngram_range)\n",
    "    vectorizer.fit(data)\n",
    "    res = vectorizer.transform(data)\n",
    "    indices = np.where(np.array(res.sum(axis = 0)).flatten() > limit)[0]\n",
    "    #names = vectorizer.get_feature_names()\n",
    "    res = res[:,indices]\n",
    "    #names = operator.itemgetter(*indices)(names)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug-in estimator of partisanship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorize the data\n",
    "data = vectorize_data(tweet_data[\"preprocessed\"],ngram_range = (2,2), limit = 10)\n",
    "\n",
    "#speaker phrase counts\n",
    "counts = dict()\n",
    "zipper = zip(tweet_data[\"source\"].tolist(), tweet_data[\"user-id_str\"].tolist(), data)\n",
    "for g, elem in itertools.groupby(zipper,lambda x: (x[0],x[1])):\n",
    "    counts[g] = sp.sparse.vstack(list(i for _, _, i in elem))\n",
    "counts = {k:np.array(v.sum(axis = 0)).flatten() for k, v in counts.items() if v.sum() > 0}\n",
    "del(data, tweet_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.780099488861822\n"
     ]
    }
   ],
   "source": [
    "#Party-wise total phrase counts\n",
    "party_counts = collections.defaultdict(lambda: [])\n",
    "for g, elem in itertools.groupby(zip(counts.keys(), counts.values()), lambda x: x[0][0]):\n",
    "    party_counts[g].extend(list(i for _,i in elem))\n",
    "\n",
    "#empirical party frequencies\n",
    "party_freq = {k:np.vstack(v).sum(axis = 0) for k, v in party_counts.items()}\n",
    "party_freq = {k:v/v.sum() for k, v in party_freq.items()}\n",
    "\n",
    "#posterior belief that an observer with neutral prior assigns the true party:\n",
    "posterior = party_freq[\"government\"]/(party_freq[\"government\"] + party_freq[\"opposition\"])\n",
    "\n",
    "\n",
    "#partisanship plug-in estimator\n",
    "plug_in = np.sum(0.5*party_freq[\"government\"] * posterior + 0.5*party_freq[\"opposition\"]*(1 - posterior))\n",
    "print(plug_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-out estimator of partisanship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mat = sp.sparse.csr_matrix(np.vstack(list(counts.values())))\n",
    "parties = np.array(list(party for party, id in counts.keys()))\n",
    "indg = parties == \"government\"\n",
    "indo = parties == \"opposition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "leave_out = collections.defaultdict(lambda: [])\n",
    "zipper = zip(counts.keys(), c_mat)\n",
    "for i, (g, val) in enumerate(itertools.groupby(zipper, lambda x:x[0])):\n",
    "    ind = np.arange(c_mat.shape[0]) != i\n",
    "    qg = np.sum(c_mat[ind & indg], axis = 0)\n",
    "    qo = np.sum(c_mat[ind & indo], axis = 0)\n",
    "    leave_out[g[0]].extend(np.array(qg/(qo+qg)))\n",
    "leave_out = {k:np.nan_to_num(np.vstack(v),copy = False,nan = 0.0) for k, v in leave_out.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_freq = collections.defaultdict(lambda: [])\n",
    "for g, elem in itertools.groupby(zip(parties, counts.values()), lambda x: x[0]):\n",
    "    speaker_freq[g].extend(list(i/i.sum() for _, i in elem))\n",
    "speaker_freq = {k:np.vstack(v) for k, v in speaker_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = 0.5*(1/len(speaker_freq[\"government\"]))*np.sum(speaker_freq[\"government\"] * leave_out[\"government\"])\n",
    "opp = 0.5*(1/len(speaker_freq[\"opposition\"]))*np.sum(speaker_freq[\"opposition\"] * (1 - leave_out[\"opposition\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675961936352196"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gov + opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPolarization:\n",
    "    def __init__(self, parties, speakers, text):\n",
    "        text = self.vectorize_data(text, ngram_range = (2,2), limit = 10) #vectorize\n",
    "        self.party = np.unique(parties) #store party names\n",
    "        \n",
    "        #get counts of each bigram for each speaker:\n",
    "        zipper = sorted(zip(parties, speakers, text), key = lambda x:(x[0], x[1]))\n",
    "        self.counts = dict()\n",
    "        for g, elem in itertools.groupby(zipper,lambda x: (x[0],x[1])):\n",
    "            self.counts[g] = sp.sparse.vstack(list(i for _, _, i in elem))\n",
    "        self.counts = {k:np.array(v.sum(axis = 0)).flatten() for k, v in self.counts.items() if v.sum() > 0}\n",
    "        \n",
    "    \n",
    "    def vectorize_data(self, data, ngram_range, limit):\n",
    "        vectorizer = CountVectorizer(ngram_range = ngram_range)\n",
    "        vectorizer.fit(data)\n",
    "        res = vectorizer.transform(data)\n",
    "        indices = np.where(np.array(res.sum(axis = 0)).flatten() > limit)[0]\n",
    "        #names = vectorizer.get_feature_names()\n",
    "        res = res[:,indices]\n",
    "        #names = operator.itemgetter(*indices)(names)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def plug_in(self, counts = None, confidence_interval = None):\n",
    "        if not counts:\n",
    "            counts = self.counts\n",
    "        #Party-wise total phrase counts\n",
    "        party_counts = collections.defaultdict(lambda: [])\n",
    "        for g, elem in itertools.groupby(zip(counts.keys(), counts.values()), lambda x: x[0][0]):\n",
    "            party_counts[g].extend(list(i for _,i in elem))\n",
    "\n",
    "        #empirical party frequencies\n",
    "        party_freq = {k:np.vstack(v).sum(axis = 0) for k, v in party_counts.items()}\n",
    "        party_freq = {k:v/v.sum() for k, v in party_freq.items()}\n",
    "\n",
    "        #posterior belief that an observer with neutral prior assigns the true party:\n",
    "        posterior = party_freq[self.party[0]]/(party_freq[self.party[0]] + party_freq[self.party[1]])\n",
    "        \n",
    "        \n",
    "        results = dict()\n",
    "\n",
    "        #partisanship plug-in estimator\n",
    "        results[\"point\"] = np.sum(0.5*party_freq[self.party[0]] * posterior + 0.5*party_freq[self.party[1]]*(1 - posterior))\n",
    "        \n",
    "        if confidence_interval:\n",
    "            results[\"samples\"] = []\n",
    "            \n",
    "            for i in trange(confidence_interval):\n",
    "                popsize = len(self.counts.keys())\n",
    "                sampsize = int(round(popsize/10))\n",
    "                inds = random.sample(range(popsize),sampsize)\n",
    "                sample_keys = [val for i, val in enumerate(self.counts.keys()) if i in inds]\n",
    "                sample = dict(zip(sample_keys, operator.itemgetter(*sample_keys)(self.counts)))\n",
    "                results[\"samples\"].append(self.plug_in(counts = sample, confidence_interval = None))\n",
    "                \n",
    "            return results\n",
    "        \n",
    "        return results[\"point\"]\n",
    "    \n",
    "    \n",
    "    def leave_out(self, counts = None, confidence_interval = None):\n",
    "        \n",
    "        if not counts:\n",
    "            counts = self.counts\n",
    "        \n",
    "        c_mat = sp.sparse.csr_matrix(np.vstack(list(counts.values())))\n",
    "        parties = np.array(list(party for party, id in counts.keys()))\n",
    "        indg = np.array(parties) == self.party[0]\n",
    "        indo = np.array(parties) == self.party[1]\n",
    "        \n",
    "        #get posterior belief of neutral observer for each phrase\n",
    "        posterior = collections.defaultdict(lambda: [])\n",
    "        zipper = zip(counts.keys(), c_mat)\n",
    "        for i, (g, val) in enumerate(itertools.groupby(zipper, lambda x:x[0])):\n",
    "            ind = np.arange(c_mat.shape[0]) != i\n",
    "            qg = np.sum(c_mat[ind & indg], axis = 0)\n",
    "            qo = np.sum(c_mat[ind & indo], axis = 0)\n",
    "            posterior[g[0]].extend(np.array(qg/(qo+qg)))\n",
    "        posterior = {k:np.nan_to_num(np.vstack(v),copy = False,nan = 0.0) for k, v in posterior.items()}\n",
    "        \n",
    "        \n",
    "        #get speaker phrase frequencies\n",
    "        speaker_freq = collections.defaultdict(lambda: [])\n",
    "        for g, elem in itertools.groupby(zip(parties, counts.values()), lambda x: x[0]):\n",
    "            speaker_freq[g].extend(list(i/i.sum() for _, i in elem))\n",
    "        speaker_freq = {k:np.vstack(v) for k, v in speaker_freq.items()}\n",
    "        \n",
    "        #results\n",
    "        results = dict()\n",
    "        \n",
    "        \n",
    "        \n",
    "        gov = 0.5*(1/len(speaker_freq[self.party[0]]))*np.sum(speaker_freq[self.party[0]] * posterior[self.party[0]])\n",
    "        opp = 0.5*(1/len(speaker_freq[self.party[1]]))*np.sum(speaker_freq[self.party[1]] * (1 - posterior[self.party[1]]))\n",
    "        results[\"point\"] = gov + opp\n",
    "        \n",
    "        if confidence_interval:\n",
    "            results[\"samples\"] = []\n",
    "            \n",
    "            for i in trange(confidence_interval):\n",
    "                popsize = len(self.counts.keys())\n",
    "                sampsize = int(round(popsize/10))\n",
    "                #pdb.set_trace()\n",
    "                inds = random.sample(range(popsize),sampsize)\n",
    "                sample_keys = [val for i, val in enumerate(self.counts.keys()) if i in inds]\n",
    "                sample = dict(zip(sample_keys, operator.itemgetter(*sample_keys)(self.counts)))\n",
    "                results[\"samples\"].append(self.leave_out(counts = sample, confidence_interval = None))\n",
    "            \n",
    "            results[\"lower\"] = np.quantile(results[\"samples\"], 0.025)\n",
    "            results[\"upper\"] = np.quantile(results[\"samples\"], 0.975)\n",
    "                \n",
    "            return results\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        return results[\"point\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 23.77it/s]\n",
      "100%|██████████| 100/100 [01:03<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "parties = tweet_data[\"source\"].tolist()\n",
    "speakers = tweet_data[\"user-id_str\"].tolist()\n",
    "text = tweet_data[\"preprocessed\"].tolist()\n",
    "model = ModelPolarization(parties, speakers, text)\n",
    "plug = model.plug_in(confidence_interval = 100)\n",
    "leave = model.leave_out(confidence_interval = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8337653844168114"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leave[\"point\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8338097711938119"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(leave[\"samples\"], 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1.,  8., 21., 30., 19., 13.,  4.,  2.]),\n",
       " array([0.74044524, 0.75083252, 0.76121979, 0.77160707, 0.78199435,\n",
       "        0.79238163, 0.8027689 , 0.81315618, 0.82354346, 0.83393073,\n",
       "        0.84431801]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(leave[\"samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models():\n",
    "    results = {\"date\":[], \"plugin\":[], \"leaveout\":[]}\n",
    "    date = pd.Period('2020-02-23', 'D') \n",
    "    for _ in trange(65):\n",
    "        date += 1\n",
    "        gov_data = pd.read_csv(\"data/clean/gov_tweets_clean.csv\", header = 0, index_col = False, dtype = str)\n",
    "        gov_data[\"source\"] = \"government\"\n",
    "        gov_data = gov_data.loc[gov_data.created_at.notna()]\n",
    "        gov_data = gov_data.drop(columns = [\"index\"])\n",
    "        gov_data[\"created_at\"] = pd.to_datetime(gov_data[\"created_at\"], format = \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "        gov_data[\"day\"] = gov_data.created_at.dt.to_period(\"D\")\n",
    "        gov_data = gov_data.loc[gov_data[\"day\"] == date].reset_index()\n",
    "\n",
    "        opp_data = pd.read_csv(\"data/clean/opp_tweets_clean.csv\", header = 0, index_col = False, dtype = str)\n",
    "        opp_data[\"source\"] = \"opposition\"\n",
    "        opp_data = opp_data.loc[opp_data.created_at.notna()]\n",
    "        opp_data = opp_data.drop(columns = [\"index\"])\n",
    "        opp_data[\"created_at\"] = pd.to_datetime(opp_data[\"created_at\"], format = \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "        opp_data[\"day\"] = opp_data.created_at.dt.to_period(\"D\")\n",
    "        opp_data = opp_data.loc[opp_data[\"day\"] == date].reset_index()\n",
    "\n",
    "\n",
    "        tweet_data = pd.concat([gov_data, opp_data], axis = 0)\n",
    "        del(gov_data)\n",
    "        del(opp_data)\n",
    "        gc.collect()\n",
    "\n",
    "        tweet_data.dropna(subset = [\"source\",\"preprocessed\"], inplace = True)\n",
    "\n",
    "        model = ModelPolarization(tweet_data[\"source\"].tolist(), tweet_data[\"user-id_str\"].tolist(), tweet_data[\"preprocessed\"].tolist())\n",
    "        results[\"date\"].append(date)\n",
    "        results[\"plugin\"].append(model.plug_in())\n",
    "        results[\"leaveout\"].append(model.leave_out())\n",
    "    results[\"date\"] = list(map(lambda x: x.strftime(\"%Y-%m-%d\"), results[\"date\"]))\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
