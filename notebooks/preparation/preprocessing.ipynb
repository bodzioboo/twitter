{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import defaultdict\n",
    "import pdb\n",
    "import gc\n",
    "from functools import partial\n",
    "import ast\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup logging:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/piotr/projects/twitter\"\n",
    "path_preprocessed = os.path.join(path, 'data/clean')\n",
    "sys.path.append(os.path.join(path, \"src\"))\n",
    "ndays = 84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing and lemmatization is performed through the `Preprocessor` class, which performs the following task:\n",
    "- ensures integrity of each record, drops it if any problems occur - for example, unexpected data types key fields containing text, user IDs or Tweet date\n",
    "- drops Tweets marked as having a different language than Polish or undefined\n",
    "- removes URLs, numbers, emojis and user mentions from each Tweet's text\n",
    "- replaces hashtags included in Tweets with words\n",
    "- marks the retweets in the dataset\n",
    "- tokenizes the text, performs POS tagging and lemmatization on each of the texts using the preprocessing Pipeline from the `stanza` library. Two separate fields are kept - `tokenized` for tokenized data after the preprocessing steps mentioned above and `lemmatized` for fully lemmatized data\n",
    "- no stopword removal is performed at this point\n",
    "- computes the proportion of Polish tokens in the data using the spell checker from the `pyenchant` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I set up the parameters for lemmatization - file directories, chunk size for the preprocessor, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['full_text', 'created_at', 'id_str', 'user-id_str'] #columns to be kept\n",
    "path_record = os.path.join(path, \"data/clean/record.json\") #path to store records\n",
    "chunk_size = 10000 #chunk size for CSV reader\n",
    "pos_batch_size = 1000 #batch size for POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_tools.preprocessing import Preprocessor\n",
    "def preprocess(path_raw, path_preprocessed, preprocessor, keep_cols, path_record = None, \n",
    "               chunk_size = 10000, pos_batch_size = 1000):\n",
    "    \"\"\"\n",
    "    Takes preprocessor as argument to avoid re-instantiation.\n",
    "    \"\"\"\n",
    "    #record:\n",
    "    if path_record and os.path.isfile(path_record):\n",
    "        record = json.load(open(path_record, \"r\"))\n",
    "        record = defaultdict(lambda: 0, record)\n",
    "    else:\n",
    "        record = defaultdict(lambda: 0)\n",
    "    names = pd.read_csv(path_raw, dtype = str, nrows = 1, index_col = 0).columns #get column name\n",
    "    for df in pd.read_csv(path_raw, iterator = True, dtype = str, \n",
    "                           skiprows = record[path_raw] + 1,\n",
    "                           chunksize = chunk_size, index_col = 0, \n",
    "                           names = names):\n",
    "        res = preprocessor.preprocess(df)\n",
    "        \n",
    "        if res is not None:\n",
    "            \n",
    "            if os.path.isfile(path_preprocessed):\n",
    "                res.to_csv(path_preprocessed, mode = \"a\", header = False) #append if exists\n",
    "            else:\n",
    "                res.to_csv(path_preprocessed, mode = \"w\") #else write to new\n",
    "\n",
    "            record[path_raw] += chunk_size\n",
    "\n",
    "            if path_record:\n",
    "                json.dump(dict(record), open(path_record, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Government Tweets - February-May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir(path_preprocessed):\n",
    "    path_raw = '/media/piotr/SAMSUNG/data/gov'\n",
    "    preprocessor = Preprocessor(keep_cols = keep_cols, pos_batch_size = pos_batch_size) #init\n",
    "    files = [f for f in os.listdir(path_raw) if 'csv' in f]\n",
    "    for file in tqdm(files):\n",
    "        preprocess(os.path.join(path_raw, file), os.path.join(path_preprocessed, file), preprocessor, \n",
    "                   keep_cols, path_record = path_record, chunk_size = 10000, pos_batch_size = 1000)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opposition Tweets - February-May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir(path_preprocessed):\n",
    "    path_raw = '/media/piotr/SAMSUNG/data/opp'\n",
    "    preprocessor = Preprocessor(keep_cols = keep_cols, pos_batch_size = pos_batch_size) #init\n",
    "    files = [f for f in os.listdir(path_raw) if 'csv' in f]\n",
    "    for file in tqdm(files):\n",
    "        preprocess(os.path.join(path_raw, file), os.path.join(path_preprocessed, file), preprocessor, \n",
    "                   keep_cols, path_record = path_record, chunk_size = 10000, pos_batch_size = 1000)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902e98966ded4464be20341edf940503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from twitter_tools.utils import read_files\n",
    "data = pd.DataFrame()\n",
    "for df in tqdm(read_files(path_preprocessed, ndays = ndays, prefixes = ['opp', 'gov'], dtype = str)):\n",
    "    data = data.append(df)\n",
    "data.loc[:,('polish')] = data.loc[:,('polish')].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Removing Tweets in a different language** - Some of the users scraped were using other languages than Polish, rendering any form of NLP analysis of their content difficult. To remove them, I analyzed the average proportion of non-Polish tokens in the lemmatized version of their tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-Polish users 53\n",
      "Example tweets:\n",
      "-fotografía photography nature naturaleza life dreams\n",
      "-odilon redon la sphere globe\n",
      "-photography nikon nikonglobal nikonphotography nikonnofilter\n",
      "-emailextractor webscraping targetedemail\n",
      "-vladimir putin odhaľuje západ ovládajú satanskí pedofili\n",
      "-just posted трамп выпускает джина из бутылки on reddit\n",
      "-china lied people died make chinapay chinazi virus\n",
      "-by marcin gorski\n",
      "-presne podľa plánu kalergi zbaviť sa inteligentejšej rasy krížením rás vytvori ť negroidniho podčloveka bez práv nároku len poslušného otroka\n",
      "-cdnpoli\n",
      "-kwsk\n",
      "-oh\n",
      "-aj tu sa ukázalo potvrdilo že demokracia liberálneko kapitalizmu je vedúca do záhuby ako to už preukázalo minulosti čo vyústilo do vojen svetových kríz kde zahyuli miliónov civilistov mene bankových zbrojných korporácií\n",
      "-aliennation\n",
      "-mjesečev put utočište\n",
      "-kraków kazimierz\n",
      "-idd jkucharski covidbc covid bcpoli cdnpoli\n",
      "-daniel volkov\n",
      "-rusi zachraňujú zadky amíkom\n",
      "-just posted как не признанных мая отмечают on reddit\n",
      "-dobranoc\n",
      "-just posted слуги народа не смогли on reddit\n",
      "-just posted пьяный священни к харькове on reddit\n",
      "-via\n",
      "-anglosasov liberalistov nezaujímajú ľudia ale len peniaze majetky chamtivosť vojny minúty\n",
      "-treptaju tvoga oka prolazi vrijeme\n",
      "-just posted он обрывал телефо н белого дома on reddit\n",
      "-just posted россия стремительно идет нищету on reddit\n",
      "-zdzislaw beksinski\n",
      "-mai berlin\n",
      "-drawing drawings art artofinstagram illustration illustrator pictureoftheday design sketch sketchbook graphicdesign graphic art artwork lines doodlesketch brush pen design designinspiration inspiration pattern brush digitalpaint painting draft drafting\n",
      "-sickof hajdu tam\n",
      "-cafouillage\n",
      "-no\n",
      "-fotografía photography blackandwhitephotography\n",
      "-passionnement jevotegoujon\n",
      "-china lied people died\n",
      "-ashleyspirals\n",
      "-just posted путин первый космосе on reddit\n",
      "-quoteoftheday\n",
      "-china ccp china lied people died covid coronaviruschina coronaviruspandemic xi virus globalpandemic china bubble globaleconomy\n",
      "-biljana radojicic\n",
      "-kraków\n",
      "-business iscard business flyer businessflyer\n",
      "-fruška\n",
      "-všetko sa dá keď sa chce ale provízie sa uz delia\n",
      "-danny mcshane\n",
      "-cc\n",
      "-yes coronavirus coronavirus pandemic coronavirus outbreak ccpvirus ccplied people died ccpcoronavirus ccpvirus coronavirus ccp community spread communitytransmission wuhan wuhan coronavius wuhan coronavirus outbreak ccpchina human disaster global pandemic\n",
      "-kurt jackson via ewa krepicz\n"
     ]
    }
   ],
   "source": [
    "path_drop = os.path.join(path, 'results/cleaning/users_drop_ids.json')\n",
    "from twitter_tools.utils import read_files\n",
    "if not os.path.isfile(path_drop):\n",
    "    #Get ids of \"nonpolish\" users, i.e. those with average number of Polish tokens below 0.3\n",
    "    polishness = data.groupby(['user-id_str']).mean()['polish']\n",
    "    drop_ids = polishness[polishness < 0.3].index.tolist()\n",
    "    print(f\"Number of non-Polish users {len(drop_ids)}\")\n",
    "    print(\"Example tweets:\")\n",
    "    tweets_nonpolish = data[data['user-id_str'].apply(lambda x: x in drop_ids)]\n",
    "    np.random.seed = 1234\n",
    "    for i in np.random.choice(np.arange(0, tweets_nonpolish.shape[0]), 50):\n",
    "        print(\"-\" + tweets_nonpolish.tokenized.iloc[i])\n",
    "else:\n",
    "    drop_ids = json.load(open(path_drop, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Removing bots/users with suspicious levels of activity** - drop users with extremely high number of daily Tweets (excluding Retweet activity). 'Extremely active' is understood as users with higher activity than the 99th quantile of the average daily number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user-id_str</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1223528125952417792</th>\n",
       "      <td>286.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28522629</th>\n",
       "      <td>168.171429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818748422866006016</th>\n",
       "      <td>153.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188644971</th>\n",
       "      <td>152.431373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899193463371182080</th>\n",
       "      <td>143.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71028646</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224765147757056002</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224467950922620928</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317925783</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203373416</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9076 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lemmatized\n",
       "user-id_str                    \n",
       "1223528125952417792  286.300000\n",
       "28522629             168.171429\n",
       "818748422866006016   153.051282\n",
       "2188644971           152.431373\n",
       "899193463371182080   143.833333\n",
       "...                         ...\n",
       "71028646               1.000000\n",
       "1224765147757056002    1.000000\n",
       "1224467950922620928    1.000000\n",
       "3317925783             1.000000\n",
       "2203373416             0.750000\n",
       "\n",
       "[9076 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop retweets\n",
    "drop = np.logical_not(data['retweet'].apply(lambda x: ast.literal_eval(x)))\n",
    "data = data.loc[drop, :]\n",
    "counts = data.groupby(['user-id_str', 'day'])['lemmatized'].count().reset_index()\n",
    "counts_avg = counts.groupby('user-id_str').agg({'lemmatized':'mean'}).sort_values('lemmatized', ascending = False)\n",
    "counts_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts_avg.plot.hist(figsize = (10, 8), legend = None, title = 'Distribution of average number of tweets per day')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the user tho tweeted 286 times per day to the list of profiles to be removed. The profile was later removed, so it seems like this could have been a bot activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user-id_str</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1223528125952417792</th>\n",
       "      <td>286.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28522629</th>\n",
       "      <td>168.171429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818748422866006016</th>\n",
       "      <td>153.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188644971</th>\n",
       "      <td>152.431373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899193463371182080</th>\n",
       "      <td>143.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71028646</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224765147757056002</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224467950922620928</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317925783</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203373416</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9076 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lemmatized\n",
       "user-id_str                    \n",
       "1223528125952417792  286.300000\n",
       "28522629             168.171429\n",
       "818748422866006016   153.051282\n",
       "2188644971           152.431373\n",
       "899193463371182080   143.833333\n",
       "...                         ...\n",
       "71028646               1.000000\n",
       "1224765147757056002    1.000000\n",
       "1224467950922620928    1.000000\n",
       "3317925783             1.000000\n",
       "2203373416             0.750000\n",
       "\n",
       "[9076 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.999th quantile of original tweet numbers 275.5798321429294\n",
      "Number of problematic ids 1\n"
     ]
    }
   ],
   "source": [
    "path_drop = os.path.join(path, 'results/cleaning/users_drop_ids.json')\n",
    "lim = counts_avg.quantile(0.99999)[0]\n",
    "print(f'99.999th quantile of original tweet numbers {lim}')\n",
    "high_activity = counts_avg[counts_avg['lemmatized'] > lim].reset_index()['user-id_str'].tolist()\n",
    "print(f'Number of problematic ids {len(high_activity)}')\n",
    "if not os.path.isfile(path_drop):\n",
    "    drop_ids = list(set(users_nonpolish + high_activity))\n",
    "    json.dump(drop_ids, open(path_drop, 'w'))\n",
    "else:\n",
    "    drop_ids = json.load(open(path_drop, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also look at any local irregularities, i.e. users exceeding the average number in each given day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id_str</th>\n",
       "      <th>day</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109068</th>\n",
       "      <td>2840954536</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109500</th>\n",
       "      <td>28522629</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123897</th>\n",
       "      <td>3233772889</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174797</th>\n",
       "      <td>4839322828</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1004342026547154944</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66569</th>\n",
       "      <td>1353653154</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156662</th>\n",
       "      <td>422173654</td>\n",
       "      <td>2020-02-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156663</th>\n",
       "      <td>422173654</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156665</th>\n",
       "      <td>422173654</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84396</th>\n",
       "      <td>2203373416</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276268 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                user-id_str         day  lemmatized\n",
       "109068           2840954536  2020-05-15         365\n",
       "109500             28522629  2020-05-02         353\n",
       "123897           3233772889  2020-05-01         344\n",
       "174797           4839322828  2020-05-15         343\n",
       "1063    1004342026547154944  2020-05-15         340\n",
       "...                     ...         ...         ...\n",
       "66569            1353653154  2020-03-24           1\n",
       "156662            422173654  2020-02-22           1\n",
       "156663            422173654  2020-02-24           1\n",
       "156665            422173654  2020-02-27           1\n",
       "84396            2203373416  2020-03-06           0\n",
       "\n",
       "[276268 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[np.logical_not(counts['user-id_str'].isin(drop_ids))].sort_values('lemmatized', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "- next, using the Sentence Embeddings method (based on fasttext vectors with 300 dimensions), I identify potential outliers in the data using the IsolationForrest algorithm and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings = '/home/piotr/nlp/cc.pl.300.vec'\n",
    "from twitter_models.embeddings import OutlierDetector\n",
    "def filter_fun(df, cols, drop_ids, drop_duplicates = True):\n",
    "    df = df[np.logical_not(df['user-id_str'].isin(drop_ids))] #drop IDs that are to be excluded\n",
    "    df = df[df.polish.astype(float) > 0.5] #don't include non-Polish\n",
    "    if drop_duplicates:\n",
    "        df.drop_duplicates(inplace = True)\n",
    "    df = df[cols]\n",
    "    return df\n",
    "ff = partial(filter_fun, cols = ['lemmatized', 'day', 'source', 'id_str'], drop_ids = drop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_outliers = os.path.join(path, 'results/cleaning/outliers.json')\n",
    "if os.path.isfile(path_outliers):\n",
    "    outliers = json.load(open(path_outliers, 'r')) #load the outlier dict\n",
    "else:\n",
    "    from twitter_tools.utils import batch\n",
    "    data = pd.DataFrame()\n",
    "    for df in tqdm(read_files(path_preprocessed, 84, prefixes = [\"opp\", \"gov\"], dtype = str, \n",
    "                              filter_fun = ff)):\n",
    "        data = data.append(df)\n",
    "    data.drop_duplicates(inplace = True) #keep only unique tweets\n",
    "    splitter = StratifiedShuffleSplit(n_splits = 1, train_size = 0.1, random_state = 1234) #get a train-test split\n",
    "    detector = OutlierDetector(path_embeddings, IsolationForest(n_jobs = 3), a = 0.001) #init detector\n",
    "    tr, ts = next(splitter.split(data, data['day'] + data['source'])) #split - stratify by date and source\n",
    "    data = data.lemmatized #keep just the lemmatized text column\n",
    "    print(f'Train size {tr.shape}')\n",
    "    detector.fit(data.iloc[tr].astype(str))\n",
    "    pickle.dump(detector, open(os.path.join(path, 'results/cleaning/outlier_detector.p'), 'wb')) #save model\n",
    "    print(f'Finished fitting. Predicting outliers')\n",
    "    preds = [] #predict\n",
    "    for i, text in tqdm(enumerate(batch(data, 500000))):\n",
    "        preds.extend(detector.predict(text.astype(str)))\n",
    "    outliers = dict(filter(lambda x: x[1] == -1, zip(data.tolist(), preds)))\n",
    "    outliers = list(outliers.keys())\n",
    "    \n",
    "    #now load the data again without dropping the duplicates\n",
    "    #and get a list of all tweet ids tagged as duplicates\n",
    "    data = pd.DataFrame()\n",
    "    ff = partial(filter_fun, cols = ['lemmatized', 'id_str'], drop_ids = drop_ids, drop_duplicates = False)\n",
    "    for df in tqdm(read_files(path_preprocessed, 84, prefixes = [\"opp\", \"gov\"], dtype = str, \n",
    "                          filter_fun = ff)):\n",
    "        data = data.append(df)\n",
    "    data['outlier'] = data.lemmatized.isin(outliers)\n",
    "    oulier_ids = data.loc[data.outlier,('id_str')].tolist()\n",
    "    json.dump(outlier_ids, open(path_outliers, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step\n",
    "Get ids of all tweets that should be removed based on the previous analysis + all with below 0.5 words in Polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_users = json.load(open(os.path.join(path, 'results/cleaning/users_drop_ids.json'), 'r'))\n",
    "outliers = json.load(open(os.path.join(path, 'results/cleaning/outliers.json'), 'r'))\n",
    "def get_ids(df, drop_users: list, drop_tweets: list):\n",
    "    drop = df['user-id_str'].isin(drop_users) #drop users that are to be excluded\n",
    "    drop |= df['id_str'].isin(drop_tweets) #drop tweets that are to be excluded\n",
    "    drop |= df.polish.astype(float) < 0.5 #drop tweets with less than 50% Polish\n",
    "    ids = df.loc[drop, ('id_str')].tolist() #get ids\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tmp = os.path.join(path, 'results/cleaning/DROP_IDS.json')\n",
    "if not os.path.isfile(path_tmp):\n",
    "    ids_drop = []\n",
    "    for df in tqdm(read_files(path_preprocessed, 84, prefixes = [\"opp\", \"gov\"], dtype = str)):\n",
    "        ids_drop.extend(get_ids(df, drop_users = drop_users, drop_tweets = outliers))\n",
    "    json.dump(ids_drop, open(path_tmp, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
