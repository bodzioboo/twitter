{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from time import sleep\n",
    "import gc\n",
    "import os\n",
    "import morfeusz2 \n",
    "import nltk\n",
    "from dask import array as da\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "from twitter_tools import tweet_stats, preprocess_tweets\n",
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from upsetplot import plot as setplot\n",
    "from tweepy import API, AppAuthHandler, Cursor\n",
    "from config import consumer_key, consumer_secret, access_token, access_secret\n",
    "from twitter_scrapers import FollowerScraper, KeywordsScraper\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\"data/clean/gov_tweets_clean_2020_02_22.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp[\"created_at\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24373"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = [i for i in tmp if re.match(\"[A-Z][a-z]{2} [A-Z][a-z]{2} \\d{2} \\d{2}:\\d{2}:\\d{2} \\+\\d{4} \\d{4}\", str(i))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24373"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24372"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da08a117351a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgovernment_followers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/government_followers.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mopposition_followers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/opposition_followers.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "government_followers = json.load(open(\"data/government_followers.json\",\"r\"))\n",
    "opposition_followers = json.load(open(\"data/opposition_followers.json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_followers(followers_dict):\n",
    "    profile_names = list(followers_dict.keys())\n",
    "    followers_intersections = [pd.Series(True, index=list(elements), name=name)\n",
    "                      for name, elements in followers_dict.items()] #set all values to true where id exists\n",
    "    followers_intersections = pd.concat(followers_intersections,sort = False,axis = 1) #concatanate\n",
    "    followers_intersections = followers_intersections.fillna(False).reset_index() #fill nas and put id to columns\n",
    "    followers_intersections = followers_intersections.groupby(profile_names).count() #get counts\n",
    "    return followers_intersections[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opposition_followers = {k:set(v) for k,v in opposition_followers.items()}\n",
    "setplot(intersect_followers(opposition_followers),show_percentages = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "government_followers = {k:set(v) for k,v in government_followers.items()}\n",
    "government_intersections = intersect_followers(government_followers)\n",
    "setplot(government_intersections,show_percentages = True)\n",
    "plt.show()\n",
    "print(pd.DataFrame(government_intersections).sort_values(by = \"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get followers of politician profiles\n",
    "The method `getFollowers` from `FollowerScrapers` given a list of Twitter account names returns dictionary of the account names from the list associatied with list of IDs of their followers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get followers of government politicians profiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/government_followers.json\"):\n",
    "    government_followers = json.load(open(\"data/government_followers.json\",\"r\"))\n",
    "else:\n",
    "    government = ['@D_Tarczynski', '@BeataSzydlo', '@Macierewicz_A', \n",
    "                  '@KrystPawlowicz', '@StKarczewski', '@MorawieckiM', \n",
    "                  '@ZiobroPL', '@jbrudzinski', '@PatrykJaki', '@mblaszczak']\n",
    "    government_followers = scraper.getFollowers(government, \"data/government_followers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get followers of opposition politicians profiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/opposition_followers.json\"):\n",
    "    opposition_followers = json.load(open(\"data/opposition_followers.json\",\"r\"))\n",
    "else:\n",
    "    opposition = ['@SchetynadlaPO', '@bbudka', '@KLubnauer', '@Arlukowicz', \n",
    "                  '@profGrodzki', '@RyszardPetru', '@trzaskowski_', \n",
    "                  '@TomaszSiemoniak', '@Gasiuk_Pihowicz']\n",
    "    opposition_followers = scraper.getFollowers(opposition, \"data/opposition_followers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow down the population to political partisans\n",
    "The method `subsetFollowers` given two followers dictionaries such as the ones created by the `getFollowers` method returns two lists of followers, each that had at least $n$ overlap within one dictionary and at most $m$ overlap with the other. Using this method I filtered out two distinct populations of users - *opposition partisans* following at least 6 opposition politicians and at most 4 government politicians and *government partisans* following at least 6 government politicians and and most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/gov_partisans.pickle\") and os.path.isfile(\"data/opp_partisans.pickle\"):\n",
    "    gov_partisans = pickle.load(open(\"data/gov_partisans.pickle\",\"rb\"))\n",
    "    opp_partisans = pickle.load(open(\"data/opp_partisans.pickle\",\"rb\"))\n",
    "else:\n",
    "    gov_partisans, opp_partisans = scraper.subsetFollowers(government_followers, opposition_followers, 6, 4)\n",
    "    pickle.dump(gov_partisans,open(\"data/gov_partisans.pickle\",\"wb\"))\n",
    "    pickle.dump(opp_partisans,open(\"data/opp_partisans.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of these groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35422 opposition partisans and 26465 government partisans were filtered\n"
     ]
    }
   ],
   "source": [
    "print(\"{} opposition partisans and {} government partisans were filtered\".format(len(gov_partisans),len(opp_partisans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there's no overlap between the two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(gov_partisans).intersection(set(opp_partisans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get population info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/gov_partisans_info.csv\"):\n",
    "    gov_followers_info = pd.read_csv(\"data/gov_partisans_info.csv\", index_col = 0)\n",
    "else:\n",
    "    scraper.getFollowersData(gov_partisans, \"data/gov_partisans_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1379/26465 [09:18<2:15:36,  3.08it/s] Rate limit reached. Sleeping for: 516\n",
      "  9%|▊         | 2279/26465 [23:17<3:19:27,  2.02it/s]    Rate limit reached. Sleeping for: 582\n",
      " 12%|█▏        | 3179/26465 [39:20<2:27:28,  2.63it/s]    Rate limit reached. Sleeping for: 524\n",
      " 15%|█▌        | 4079/26465 [53:24<2:00:31,  3.10it/s]    Rate limit reached. Sleeping for: 586\n",
      " 19%|█▉        | 4979/26465 [1:08:27<2:05:22,  2.86it/s]    Rate limit reached. Sleeping for: 588\n",
      " 22%|██▏       | 5879/26465 [1:23:34<2:04:00,  2.77it/s]    Rate limit reached. Sleeping for: 586\n",
      " 26%|██▌       | 6779/26465 [1:39:41<2:07:07,  2.58it/s]    Rate limit reached. Sleeping for: 524\n",
      " 29%|██▉       | 7679/26465 [1:53:46<2:02:39,  2.55it/s]   Rate limit reached. Sleeping for: 584\n",
      " 32%|███▏      | 8579/26465 [2:08:49<1:54:47,  2.60it/s]   Rate limit reached. Sleeping for: 586\n",
      " 36%|███▌      | 9479/26465 [2:24:04<1:38:01,  2.89it/s]   Rate limit reached. Sleeping for: 576\n",
      " 39%|███▉      | 10379/26465 [2:39:10<1:30:06,  2.98it/s]  Rate limit reached. Sleeping for: 575\n",
      " 43%|████▎     | 11279/26465 [2:54:18<1:37:16,  2.60it/s]   Rate limit reached. Sleeping for: 571\n",
      " 46%|████▌     | 12179/26465 [3:09:19<1:22:54,  2.87it/s]   Rate limit reached. Sleeping for: 576\n",
      " 49%|████▉     | 13079/26465 [3:24:25<1:18:12,  2.85it/s]   Rate limit reached. Sleeping for: 575\n",
      " 53%|█████▎    | 13979/26465 [3:40:27<1:12:07,  2.88it/s]   Rate limit reached. Sleeping for: 518\n",
      " 56%|█████▌    | 14879/26465 [3:54:32<1:03:11,  3.06it/s]   Rate limit reached. Sleeping for: 577\n",
      " 60%|█████▉    | 15779/26465 [4:12:38<1:03:45,  2.79it/s]   Rate limit reached. Sleeping for: 397\n",
      " 63%|██████▎   | 16679/26465 [4:25:41<1:07:46,  2.41it/s]   Rate limit reached. Sleeping for: 519\n",
      " 66%|██████▋   | 17579/26465 [4:40:43<50:14,  2.95it/s]     Rate limit reached. Sleeping for: 522\n",
      " 70%|██████▉   | 18479/26465 [4:54:48<48:11,  2.76it/s]     Rate limit reached. Sleeping for: 582\n",
      " 73%|███████▎  | 19379/26465 [5:09:52<40:45,  2.90it/s]     Rate limit reached. Sleeping for: 582\n",
      " 77%|███████▋  | 20279/26465 [5:26:07<45:51,  2.25it/s]     Rate limit reached. Sleeping for: 513\n",
      " 80%|████████  | 21179/26465 [5:42:54<49:39,  1.77it/s]     Rate limit reached. Sleeping for: 411\n",
      " 83%|████████▎ | 22079/26465 [5:57:12<37:58,  1.92it/s]     Rate limit reached. Sleeping for: 458\n",
      " 87%|████████▋ | 22979/26465 [6:12:20<28:32,  2.04it/s]     Rate limit reached. Sleeping for: 455\n",
      " 90%|█████████ | 23879/26465 [6:27:24<22:15,  1.94it/s]     Rate limit reached. Sleeping for: 456\n",
      " 94%|█████████▎| 24779/26465 [6:44:16<13:20,  2.11it/s]    Rate limit reached. Sleeping for: 350\n",
      " 97%|█████████▋| 25679/26465 [6:56:09<04:55,  2.66it/s]    Rate limit reached. Sleeping for: 542\n",
      "100%|██████████| 26465/26465 [7:10:03<00:00,  1.03it/s]    \n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"data/opp_partisans_info.csv\"):\n",
    "    opp_followers_info = pd.read_csv(\"data/opp_partisans_info.csv\", index_col = 0)\n",
    "else:\n",
    "    scraper.getFollowersData(opp_partisans, \"data/opp_partisans_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples:\n",
    "Given the above partisans, I sampled 5000 from each group that met the following constraints:\n",
    "- every one had at least 1000 tweets in life time\n",
    "- they were active within last 7 days (01.04.2020) \n",
    "- the Twitter language was \"pl\" or \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/gov_sample.pickle\"):\n",
    "    gov_sample = pickle.load(open(\"data/gov_sample.pickle\",\"rb\"))\n",
    "else:\n",
    "    gov_sample = scraper.sampleFollowers(gov_partisans, num_users = 5000, min_usr_tweets = 1000, \n",
    "                            last_date_active = datetime.datetime.now() - datetime.timedelta(days = 7),\n",
    "                           lang = [None,\"pl\"])\n",
    "    pickle.dump(gov_sample,open(\"data/gov_sample.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/opp_sample.pickle\"):\n",
    "    opp_sample = pickle.load(open(\"data/opp_sample.pickle\",\"rb\"))\n",
    "else:\n",
    "    opp_sample = scraper.sampleFollowers(opp_partisans, num_users = 5000, min_usr_tweets = 1000, \n",
    "                            last_date_active = datetime.datetime.now() - datetime.timedelta(days = 7),\n",
    "                           lang = [None,\"pl\"])\n",
    "    pickle.dump(opp_sample,open(\"data/opp_sample.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = FollowerScraper(api = api, path_log = \"test/scraper_test.log\")\n",
    "twitter_columns = pickle.load(open(\"data/twitter_columns.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape\n",
    "Scraping ids selected based on the above conditions that were active in March ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_ids = pickle.load(open(\"data/sample/gov_ids.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load full (approximately) list of possible columns that can be scraped:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_columns = pickle.load(open(\"twitter_columns.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize scraper:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scraper = FollowerScraper(api = api, path_log = \"scraper.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_ids = pickle.load(open(\"data/sample/gov_ids.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 144/5000 [25:24<53:26:43, 39.62s/it]Rate limit reached. Sleeping for: 62\n",
      "  4%|▍         | 197/5000 [40:20<20:53:01, 15.65s/it]Rate limit reached. Sleeping for: 68\n",
      "  5%|▍         | 247/5000 [55:09<25:57:02, 19.66s/it]Rate limit reached. Sleeping for: 80\n",
      "  8%|▊         | 378/5000 [1:25:43<14:58:32, 11.66s/it]Rate limit reached. Sleeping for: 60\n",
      "  9%|▉         | 463/5000 [1:40:53<8:13:00,  6.52s/it] Rate limit reached. Sleeping for: 48\n",
      " 10%|█         | 512/5000 [1:55:41<29:58:58, 24.05s/it]Rate limit reached. Sleeping for: 29\n",
      "100%|██████████| 5000/5000 [21:33:22<00:00, 15.52s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets scraped 1692598\n"
     ]
    }
   ],
   "source": [
    "scraper.scrape(gov_ids[0:5000],path = \"data/gov_tweets\",\n",
    "               min_date = datetime.datetime.strptime(\"22/02/2020\",\"%d/%m/%Y\"),\n",
    "               limit = 50, cols = twitter_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [5:59:28<00:00,  4.31s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets scraped 63269\n"
     ]
    }
   ],
   "source": [
    "scrape_from = datetime.datetime.now() - datetime.timedelta(days = 5)\n",
    "scraper.scrape(gov_ids[0:5000],path = \"data/scraped/gov_tweets\",\n",
    "               min_date = scrape_from,\n",
    "               limit = 50, cols = twitter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp_ids = pickle.load(open(\"data/sample/opp_ids.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.scrape(opp_ids[0:5000], path = \"data/opp_tweets\",\n",
    "               min_date = datetime.datetime.strptime(\"22/02/2020\",\"%d/%m/%Y\"), \n",
    "               limit = 50, cols = twitter_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [5:40:38<00:00,  4.09s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets scraped 52434\n"
     ]
    }
   ],
   "source": [
    "scrape_from = datetime.datetime.now() - datetime.timedelta(days = 5)\n",
    "scraper.scrape(opp_ids[:5000], path = \"data/scraped/opp_tweets\",\n",
    "               min_date = scrape_from, \n",
    "               limit = 50, cols = twitter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fala sprzeciwu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets scraped 115\n"
     ]
    }
   ],
   "source": [
    "fala_sprzeciwu = \"#falasprzeciwu OR #FalaSprzeciwu OR #FalaSprzeciwuLive OR #CzarnyProtest OR #czarnyprotest\"\n",
    "scraper = KeywordsScraper(api = api, path_log = \"scraper_fala.log\")\n",
    "scraper.scrape(fala_sprzeciwu, min_date = datetime.datetime.now() - datetime.timedelta(days = 14), \n",
    "               path = \"data/fala\", limit = 50, cols = twitter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:00<00:00, 72.09s/it]\n",
      "100%|██████████| 10/10 [10:46<00:00, 64.67s/it]\n",
      "100%|██████████| 10/10 [10:41<00:00, 64.13s/it]\n",
      "100%|██████████| 10/10 [12:11<00:00, 73.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'    \\ngov_partisans = np.array(government_superset)[(gov_in_gov >= 6) & (gov_in_opp <= 4)].tolist()\\nopp_partisans = np.array(opposition_superset)[(opp_in_opp >= 6) & (opp_in_gov <= 4)].tolist()\\n\\n\\npickle.dump(gov_partisans, open(\"data/gov_partisans.pickle\",\"wb\"))\\npickle.dump(opp_partisans, open(\"data/opp_partisans.pickle\",\"wb\"))\\n\\n\\n#no overlap\\nset(gov_partisans).intersection(set(opp_partisans))\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to filter out elements of superset that occur in at most/least n subsets\n",
    "def how_many(superset,subsets):\n",
    "    superset = np.array(superset)[:,np.newaxis] #convert to Nx1 nparray\n",
    "    superset = da.from_array(superset, chunks = (2000,1)) #convert to dask\n",
    "    count = np.zeros(superset.shape[0]).astype(\"int8\")\n",
    "    for subset in tqdm(subsets):\n",
    "        tmp = da.from_array(subset)\n",
    "        count += (superset == tmp).sum(axis = 1).compute().astype(\"int8\")\n",
    "        gc.collect()\n",
    "    return count\n",
    "\n",
    "government_superset = list(set(itertools.chain.from_iterable(government_followers.values())))\n",
    "government_subsets = list(government_followers.values())\n",
    "opposition_superset = list(set(itertools.chain.from_iterable(opposition_followers.values())))\n",
    "opposition_subsets = list(opposition_followers.values())\n",
    "\n",
    "\n",
    "gov_in_gov = how_many(government_superset, government_subsets) #how many gov profiles followed by each gov follower\n",
    "gov_in_opp = how_many(government_superset, opposition_subsets) #how many opp profiles followed by each gov follower\n",
    "opp_in_opp = how_many(opposition_superset, opposition_subsets) #how many opp profiles followed by each opp follower\n",
    "opp_in_gov = how_many(opposition_superset, government_subsets) #how many gov profiles followed by each opp follower\n",
    "pickle.dump([gov_in_gov,gov_in_opp,opp_in_opp,opp_in_gov], open(\"overlaps.pickle\",\"wb\"))\n",
    "\n",
    "    \n",
    "\"\"\"    \n",
    "gov_partisans = np.array(government_superset)[(gov_in_gov >= 6) & (gov_in_opp <= 4)].tolist()\n",
    "opp_partisans = np.array(opposition_superset)[(opp_in_opp >= 6) & (opp_in_gov <= 4)].tolist()\n",
    "\n",
    "\n",
    "pickle.dump(gov_partisans, open(\"data/gov_partisans.pickle\",\"wb\"))\n",
    "pickle.dump(opp_partisans, open(\"data/opp_partisans.pickle\",\"wb\"))\n",
    "\n",
    "\n",
    "#no overlap\n",
    "set(gov_partisans).intersection(set(opp_partisans))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_partisans = np.array(government_superset)[(gov_in_gov > gov_in_opp)].tolist()\n",
    "opp_partisans = np.array(opposition_superset)[(opp_in_opp > opp_in_gov)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452429"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gov_partisans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440215"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opp_partisans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm test/gov_tweets.json\n",
    "!rm test/gov_tweets.csv\n",
    "del(scraper)\n",
    "auth = AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "scraper = FollowerScraper(api = api, path_log = \"test/scraper_test.log\")\n",
    "scraper.scrape(gov_ids[0:10],path = \"test/gov_tweets\",\n",
    "               min_date = datetime.datetime.strptime(\"28/04/2020\",\"%d/%m/%Y\"),\n",
    "               limit = 50, cols = twitter_columns)\n",
    "temp = json.load(open(\"test/gov_tweets.json\",\"r\"))\n",
    "print(len(temp['3257053341']))\n",
    "temp['3257053341'] = test\n",
    "json.dump(temp,open(\"test/gov_tweets.json\",\"w\"))\n",
    "print(len(temp['3257053341']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
